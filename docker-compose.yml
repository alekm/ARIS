services:
  redis:
    container_name: aris-redis
    image: redis:7-alpine
    # Redis port removed from host exposure for security
    # Services access via internal Docker network
    volumes:
      - ./data/redis:/data
    command: redis-server --appendonly yes
    restart: unless-stopped
    healthcheck:
      test:
        - CMD
        - redis-cli
        - ping
      interval: 10s
      timeout: 5s
      retries: 5
  audio-capture:
    container_name: aris-audio-capture
    build:
      context: .
      dockerfile: ./services/audio-capture/Dockerfile
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./data/audio:/data/audio
      - ./services/audio-capture/config.yaml:/app/config.yaml
    environment:
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - MODE=${MODE:-kiwi} # mock or kiwi
      - KIWI_HOST=${KIWI_HOST:-172.16.250.99}
      - KIWI_PORT=${KIWI_PORT:-8073}
      - KIWI_PASSWORD=${KIWI_PASSWORD:-}
      - FREQUENCY_HZ=${FREQUENCY_HZ:-7188000}
      - DEMOD_MODE=${DEMOD_MODE:-LSB}
      - DEBUG_KIWI=${DEBUG_KIWI:-false}
    restart: unless-stopped
  stt:
    container_name: aris-stt
    build:
      context: .
      dockerfile: ./services/stt/Dockerfile
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./data/transcripts:/data/transcripts
    environment:
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - MODEL_SIZE=${MODEL_SIZE:-medium} # tiny, base, small, medium, large-v2, large-v3
      - DEVICE=${DEVICE:-cuda} # cuda or cpu
      - VAD_THRESHOLD=${VAD_THRESHOLD:-0.5}
      - ENERGY_THRESHOLD=${ENERGY_THRESHOLD:-0.01}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids:
                - "1"
              capabilities:
                - gpu
    restart: unless-stopped
  callsign-extractor:
    container_name: aris-callsign-extractor
    build:
      context: .
      dockerfile: ./services/callsign-extractor/Dockerfile
    depends_on:
      redis:
        condition: service_healthy
    environment:
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
    restart: unless-stopped
  summarizer:
    container_name: aris-summarizer
    build:
      context: .
      dockerfile: ./services/summarizer/Dockerfile
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./data/summaries:/data/summaries
    environment:
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - LLM_BACKEND=${LLM_BACKEND:-ollama} # ollama, llama-cpp, or openai-compatible
      - LLM_MODEL=${LLM_MODEL:-llama3.2:latest}
      - LLM_HOST=${LLM_HOST:-host.docker.internal:11434} # Adjust for your LLM server
      - LLM_API_KEY=${LLM_API_KEY:-}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
  api:
    container_name: aris-api
    build:
      context: .
      dockerfile: ./services/api/Dockerfile
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - ${API_PORT:-8000}:8000
    volumes:
      - ./data/db:/data/db
    environment:
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - DATABASE_URL=${DATABASE_URL:-sqlite:////data/db/aris.db}
      - API_PORT=8000
      - API_HOST=${API_HOST:-0.0.0.0}
    restart: unless-stopped
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:8000/api/stats
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
volumes:
  redis-data: null
networks: {}
